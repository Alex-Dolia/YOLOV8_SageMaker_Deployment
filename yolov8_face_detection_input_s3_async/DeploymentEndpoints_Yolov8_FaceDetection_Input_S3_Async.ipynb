{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4cf7f4-9a75-41ce-afa9-73ac9d4712e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: omegaconf==2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from omegaconf==2.3.0) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from omegaconf==2.3.0) (6.0.1)\n",
      "Requirement already satisfied: ultralytics==8.1.45 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (8.1.45)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (3.8.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (1.12.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics==8.1.45) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.45) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.1.45) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.45) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.45) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.45) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.45) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.45) (2024.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.45) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.45) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.1.45) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install omegaconf==2.3.0\n",
    "!pip install ultralytics==8.1.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d266233e-49b8-4725-9f3a-6688df6693a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nvgpu==0.9.0\n",
    "#pynvml==11.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e25aca-d7fc-4f72-8c0a-b53d4591702c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing model_fn from inference.py ...\n",
      "request_body afre decode:  content-recognition-ci|sagemaker/FaceClusterDataset/FaceClusteringDataset/00c28dbb-2ba2-4625-815d-1d64849c7f7e/|frame226.jpg|input\n",
      "Executing predict_fn from inference.py ...\n",
      "\n",
      "0: 640x640 1 face, 5.4ms\n",
      "Speed: 5.0ms preprocess, 5.4ms inference, 780.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Executing output_fn from inference.py ...\n",
      "status:  !!! The directory input) does exist: True| The file input/frame226.jpg does exist: True\n",
      "boxes:  [[109.56486511230469, 27.41848373413086, 184.64451599121094, 175.40426635742188, 0.3425535261631012, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "from src import inference as inf\n",
    "import boto3\n",
    "import json\n",
    "model_dir = \"\"\n",
    "model = inf.model_fn(model_dir)\n",
    "#####################################################\n",
    "s3_bucket = \"PUT HERE BUCKET NAME IN S3\"\n",
    "s3_key = \"PUT HERE PATH TO IMAGE IN S3\"\n",
    "image_file_name = \"frame226.jpg\"\n",
    "local_dir = 'input'\n",
    "payload_tuple = (s3_bucket, s3_key, image_file_name, local_dir)\n",
    "payload = \"|\".join(payload_tuple).encode(\"utf-8\")\n",
    "#####################################################\n",
    "content_type = 'text/csv'\n",
    "input_data = inf.input_fn(payload, content_type)\n",
    "prediction_output = inf.predict_fn(input_data, model)\n",
    "str_result = inf.output_fn(prediction_output, content_type)\n",
    "result = json.loads(str_result)\n",
    "print(\"status: \", result[\"status\"])\n",
    "print(\"boxes: \", result[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7d8116-5c66-4e95-82d5-24bfc54336bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os, sagemaker, subprocess, boto3\n",
    "from datetime import datetime\n",
    "from sagemaker import s3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e630f-d8dc-47f0-818d-4ec7f20753a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250de41b-f47b-47e7-b34a-44161193bd48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/yolov8_face_detection_input_s3_async\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4530ea-f8b6-474a-8d7d-2df44beab5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the model artifacts \n",
    "bashCommand = \"tar -czf  model.tar.gz src model\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bebe5c-f2d1-425a-9f28-323b48d2cd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "model_name = \"yolov8n-face.pt\"\n",
    "model_data = 'model.tar.gz'\n",
    "model = PyTorchModel(entry_point='inference.py',\n",
    "                     model_data=model_data, \n",
    "                     framework_version='1.12', \n",
    "                     py_version='py38',\n",
    "                     role=role,\n",
    "                     source_dir = \"src\",\n",
    "                     env={'TS_MAX_RESPONSE_SIZE':'20000000', 'YOLOV8_MODEL': model_name},\n",
    "                     sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f373623c-34ad-429b-b37d-2308ea67a63f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ENDPOINT_NAME' (str)\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_NAME = 'yolov8-pytorch-facedetection-ADOLIA' + str(datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f'))\n",
    "\n",
    "# Store the endpoint name in the history to be accessed by 2_TestEndpoint.ipynb notebook\n",
    "# We dnt have _2_TestEndpoint.ipynb in this project\n",
    "%store ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1419a6-db87-4e31-b22b-2804d00f8c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov8-pytorch-facedetection-ADOLIA2024-06-12-14-10-14-954785'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60738f3-c08d-485a-a0dd-48fdf61e0c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDPOINT_NAME:  yolov8-pytorch-facedetection-ADOLIA2024-06-12-14-10-14-954785\n",
      "INSTANCE_TYPE:  ml.g5.xlarge\n",
      "---------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html\n",
    "INSTANCE_TYPE = 'ml.g5.xlarge'\n",
    "print(\"ENDPOINT_NAME: \", ENDPOINT_NAME)\n",
    "print(\"INSTANCE_TYPE: \", INSTANCE_TYPE)\n",
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "bucket = \"PUT HERE BUCKET NAME IN S3\"\n",
    "prefix = \"PUT HERE LOCATION PATH\"\n",
    "\n",
    "# Create an empty AsyncInferenceConfig object to use default values\n",
    "async_config = AsyncInferenceConfig(output_path = f\"s3://{bucket}/{prefix}/output\")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = model.deploy(async_inference_config = async_config,\n",
    "                         initial_instance_count = 1,\n",
    "                         instance_type          = INSTANCE_TYPE,\n",
    "                         #serializer             = CSVSerializer,\n",
    "                         #deserializer           = JSONDeserializer(),\n",
    "                         endpoint_name          = ENDPOINT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5431e-0022-4d71-a889-5fa6a40b29fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a9f7fc-1787-4ccf-b0b8-95e23cc253bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov8-pytorch-facedetection-ADOLIA2024-06-12-14-10-14-954785'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32049c-82df-4312-9e05-a581326eb1f3",
   "metadata": {},
   "source": [
    "# Invoke Endpoint: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5530835-f122-4a74-be1c-ec95e09e2ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Name: yolov8-pytorch-facedetection-ADOLIA2024-06-12-14-10-14-954785\n",
      "Endpoint Status = InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#%store -r ENDPOINT_NAME\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "# Restore the endpoint name stored in the 2_DeployEndpoint.ipynb notebook\n",
    "print(f'Endpoint Name: {ENDPOINT_NAME}')\n",
    "\n",
    "endpoint_created = False\n",
    "while True:\n",
    "    response = sm_client.list_endpoints()\n",
    "    for ep in response['Endpoints']:\n",
    "        print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n",
    "        if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n",
    "            endpoint_created = True\n",
    "            break\n",
    "    if endpoint_created:\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e33d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "s3_bucket = \"PUT HERE BUCKET NAME IN S3\"\n",
    "s3_key = \"PUT HERE PATH TO S3\"\n",
    "image_file_name = \"frame226.jpg\"\n",
    "# it does not work if you define local_dir = \"input\", it has to be local_dir = '/opt/ml/input'\n",
    "local_dir = '/opt/ml/input'\n",
    "payload_tuple = (s3_bucket, s3_key, image_file_name, local_dir)\n",
    "payload = \"|\".join(payload_tuple)\n",
    "#####################################################\n",
    "s3_client = boto3.client('s3')\n",
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "bucket_name = \"PUT HERE BUCKET NAME IN S3\" # sm_session.default_bucket()\n",
    "file_name = \"payload/yolov8-face-detection/\" + image_file_name.split(\".\")[0] + \".json\"\n",
    "\n",
    "# Upload the JSON string as a file to S3\n",
    "s3_client.put_object(Body=payload, Bucket=bucket_name, Key=file_name)\n",
    "s3_payload_location = f\"s3://{bucket_name}/{file_name}\"\n",
    "print(f\"File saved to {s3_payload_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1e84b-c214-4f4e-a6bc-c03b479f7647",
   "metadata": {},
   "source": [
    "# Asynchronous Inference. <br>\n",
    "There are two methods that give the same results:<br>\n",
    "1) using predict_async (please set isPredict = True); and <br>\n",
    "2) invoke_endpoint_async (set isPredict = False).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339b45f3-2c59-40cf-87fd-1cc62715ad39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isPredict = False\n",
    "if isPredict:\n",
    "   res = predictor.predict_async(input_path=s3_payload_location)\n",
    "   res_s3_output_location = res.output_path\n",
    "   res_s3_error_location11  = res.failure_path \n",
    "   print(f\"AWS S3 uploaded inference responses location: {res_s3_output_location11}\")\n",
    "   print(f\"AWS S3 failed requests errors : {res_s3_error_location}\")\n",
    "else:\n",
    "   print(\"ENDPOINT_NAME: \", ENDPOINT_NAME)\n",
    "   print(\"s3_payload_location: \", s3_payload_location)\n",
    "   boto_session = boto3.session.Session()\n",
    "   sm_runtime = boto_session.client(\"sagemaker-runtime\")\n",
    "   response = sm_runtime.invoke_endpoint_async(\n",
    "     EndpointName = ENDPOINT_NAME, \n",
    "     InputLocation= s3_payload_location\n",
    "   )\n",
    "   res_s3_output_location = response[\"OutputLocation\"]\n",
    "   print(f\"OutputLocation: {res_s3_output_location}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95abbfe2-bff4-4b89-96c7-0e5e148d1a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDPOINT_NAME:  yolov8-pytorch-facedetection-ADOLIA2024-06-12-14-10-14-954785\n",
      "s3_payload_location:  s3://content-recognition-models/payload/yolov8-face-detection/frame226.json\n",
      "OutputLocation: s3://content-recognition-models/face-tracker-response/output/9a4f2bab-7658-42ca-956b-dfebe77d80d0.out\n"
     ]
    }
   ],
   "source": [
    "if not isPredict:\n",
    "   print(\"ENDPOINT_NAME: \", ENDPOINT_NAME)\n",
    "   print(\"s3_payload_location: \", s3_payload_location)\n",
    "   boto_session = boto3.session.Session()\n",
    "   sm_runtime = boto_session.client(\"sagemaker-runtime\")\n",
    "   response = sm_runtime.invoke_endpoint_async(\n",
    "     EndpointName = ENDPOINT_NAME, \n",
    "     InputLocation= s3_payload_location\n",
    "   )\n",
    "   res_s3_output_location = response[\"OutputLocation\"]\n",
    "   print(f\"OutputLocation: {res_s3_output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb548d-d490-4aa3-baaf-1c055c4c407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecac5823-fa50-447b-8851-fb5c22fa8793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib, time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sm_session.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53839b0a-4d1e-446c-8b87-b42cd6d973a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output status: !!! The directory /opt/ml/input) does exist: True| The file /opt/ml/input/frame226.jpg does exist: True\n",
      "Output boxes: [[109.56016540527344, 27.404191970825195, 184.64613342285156, 175.42283630371094, 0.3421856760978699, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "result_ad = get_output(res_s3_output_location)\n",
    "result_ad = json.loads(result_ad)\n",
    "status = result_ad[\"status\"]\n",
    "boxes =  result_ad[\"boxes\"]\n",
    "print(f\"Output status: {status}\")\n",
    "print(f\"Output boxes: {boxes}\")\n",
    "orig_image = result_ad[\"orig_image\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901523ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# original image that I download from s3 has already bounding box (largest bounding box) and\n",
    "# face detector add another bounding box (smaller box)\n",
    "\n",
    "orig_image = np.array(result_ad[\"orig_image\"]).astype(np.uint8)\n",
    "\n",
    "image_height, image_width, _ = orig_image.shape\n",
    "model_height, model_width = 300, 300\n",
    "x_ratio = image_width/model_width\n",
    "y_ratio = image_height/model_height\n",
    "\n",
    "if 'boxes' in result_ad:\n",
    "    for idx,(x1,y1,x2,y2,conf,lbl) in enumerate(result_ad['boxes']):\n",
    "        print(\"x1,y1,x2,y2: \", x1,y1,x2,y2)\n",
    "        # Draw Bounding Boxes\n",
    "        x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n",
    "        y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n",
    "        color = (random.randint(10,255), random.randint(10,255), random.randint(10,255))\n",
    "        cv2.rectangle(orig_image, (x1,y1), (x2,y2), color, 4)\n",
    "        print(\"left top point: \", (x1,y1), \", right bottom point: \", (x2,y2))\n",
    "        print(\"detected object class: \", lbl)\n",
    "        print(\"detection confidence: \", conf)\n",
    "        cv2.putText(orig_image, f\"Class: {int(lbl)}\", (x1,y1-40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(orig_image, f\"Conf: {int(conf*100)}\", (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5a748-1d18-4ae2-a9f5-1f45d5a99692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2da65f-807a-4cf2-975a-3025ad978b3d",
   "metadata": {},
   "source": [
    "To make a compressed tar ball of the current directory <br>\n",
    "tar -czvf yolov8_face_detection_input_s3_realtime.tar.gz .  <br>\n",
    "Unzip: <br>\n",
    "tar -xzvf yolov8_face_detection_input_s3_realtime.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fb8ea-c00c-47d7-911f-8db36be4c13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
